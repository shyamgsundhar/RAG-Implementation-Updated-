{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28",
      "authorship_tag": "ABX9TyOKxvMJOlzcwtU4WR1Ok1Mr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shyamgsundhar/RAG-Implementation-Updated-/blob/main/Rag_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 746
        },
        "id": "poGzmHjcyN_U",
        "outputId": "5a0327d9-bc6d-44d8-a0d6-f5747f6989dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ“„ Reading PDF: doc.pdf\n",
            "ðŸ“Š Total pages: 15\n",
            "ðŸ“Š Total text length: 10,955 characters\n",
            "ðŸ“Š Average characters per page: 730\n",
            "Created 28 chunks\n",
            "ðŸ”„ Getting embeddings from Gemini...\n",
            "Processing 1/28\n",
            "Processing 2/28\n",
            "Processing 3/28\n",
            "Processing 4/28\n",
            "Processing 5/28\n",
            "Processing 6/28\n",
            "Processing 7/28\n",
            "Processing 8/28\n",
            "Processing 9/28\n",
            "Processing 10/28\n",
            "Processing 11/28\n",
            "Processing 12/28\n",
            "Processing 13/28\n",
            "Processing 14/28\n",
            "Processing 15/28\n",
            "Processing 16/28\n",
            "Processing 17/28\n",
            "Processing 18/28\n",
            "Processing 19/28\n",
            "Processing 20/28\n",
            "Processing 21/28\n",
            "Processing 22/28\n",
            "Processing 23/28\n",
            "Processing 24/28\n",
            "Processing 25/28\n",
            "Processing 26/28\n",
            "Processing 27/28\n",
            "Processing 28/28\n",
            "Creating FAISS index...\n",
            "Saving to files...\n",
            "Vector database created successfully!\n",
            "Files saved: vectors.index, chunks.pkl\n",
            "Vector shape: (28, 768)\n",
            "ðŸ”¢ Sample vector (first 5 dims): [ 0.03128196 -0.06782899 -0.07025576  0.00207815  0.03874438]\n",
            "\n",
            "ðŸŽ‰ Setup complete! Now you can run 'ask_questions.py' to chat with your PDF!\n"
          ]
        }
      ],
      "source": [
        "import faiss\n",
        "import google.generativeai as genai\n",
        "import PyPDF2\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "\n",
        "# Gemini API key\n",
        "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\", \"AIzaSyAYQusmLa1utpkXk6pBDD6nTePsv9aQIt4\"))\n",
        "\n",
        "\n",
        "def pdf_to_vectors(pdf_path):\n",
        "    # Read PDF\n",
        "    print(f\"ðŸ“„ Reading PDF: {pdf_path}\")\n",
        "    with open(pdf_path, 'rb') as f:\n",
        "        pdf_reader = PyPDF2.PdfReader(f)\n",
        "        total_pages = len(pdf_reader.pages)\n",
        "\n",
        "        # Extract text from each page separately\n",
        "        page_texts = []\n",
        "        for page_num, page in enumerate(pdf_reader.pages):\n",
        "            page_text = page.extract_text()\n",
        "            page_texts.append({\n",
        "                'text': page_text,\n",
        "                'page_number': page_num + 1\n",
        "            })\n",
        "\n",
        "        # Combine all text for chunking\n",
        "        text = ''.join([p['text'] for p in page_texts if p['text']])\n",
        "\n",
        "    print(f\"ðŸ“Š Total pages: {total_pages}\")\n",
        "    print(f\"ðŸ“Š Total text length: {len(text):,} characters\")\n",
        "    print(f\"ðŸ“Š Average characters per page: {len(text) // total_pages:,}\")\n",
        "\n",
        "    # Create chunks with page tracking\n",
        "    chunks = []\n",
        "    chunk_metadata = []\n",
        "\n",
        "    for i in range(0, len(text), 400):\n",
        "        chunk_text = text[i:i + 500]\n",
        "        chunks.append(chunk_text)\n",
        "\n",
        "        # Estimate which page this chunk belongs to\n",
        "        estimated_page = min((i // (len(text) // total_pages)) + 1, total_pages)\n",
        "        chunk_metadata.append({\n",
        "            'start_pos': i,\n",
        "            'estimated_page': estimated_page\n",
        "        })\n",
        "\n",
        "    print(f\"Created {len(chunks)} chunks\")\n",
        "\n",
        "    # Get embeddings from Gemini\n",
        "    print(\"ðŸ”„ Getting embeddings from Gemini...\")\n",
        "    embeddings = []\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        print(f\"Processing {i + 1}/{len(chunks)}\")\n",
        "        try:\n",
        "            response = genai.embed_content(\n",
        "                model=\"models/embedding-001\",\n",
        "                content=chunk,\n",
        "                output_dimensionality=1536\n",
        "            )\n",
        "            embeddings.append(response['embedding'])\n",
        "        except Exception as e:\n",
        "            print(f\"Error embedding chunk {i}: {e}\")\n",
        "            embeddings.append([0.0] * 1536)\n",
        "\n",
        "    # Create FAISS index\n",
        "    print(\"Creating FAISS index...\")\n",
        "    embeddings = np.array(embeddings)\n",
        "    index = faiss.IndexFlatIP(embeddings.shape[1])\n",
        "    index.add(embeddings.astype('float32'))\n",
        "\n",
        "    # Save to files\n",
        "    print(\"Saving to files...\")\n",
        "    faiss.write_index(index, \"vectors.index\")\n",
        "    with open(\"chunks.pkl\", \"wb\") as f:\n",
        "        pickle.dump({\n",
        "            'chunks': chunks,\n",
        "            'metadata': chunk_metadata,\n",
        "            'total_pages': total_pages\n",
        "        }, f)\n",
        "\n",
        "    print(\"Vector database created successfully!\")\n",
        "    print(f\"Files saved: vectors.index, chunks.pkl\")\n",
        "    print(f\"Vector shape: {embeddings.shape}\")\n",
        "    print(f\"ðŸ”¢ Sample vector (first 5 dims): {embeddings[0][:5]}\")\n",
        "\n",
        "    return embeddings, chunks\n",
        "\n",
        "\n",
        "# Usage\n",
        "if __name__ == \"__main__\":\n",
        "    pdf_file = \"doc.pdf\"  # Change to your PDF file\n",
        "    embeddings, chunks = pdf_to_vectors(pdf_file)\n",
        "\n",
        "    print(\"\\nðŸŽ‰ Setup complete! Now you can run 'ask_questions.py' to chat with your PDF!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ask_question(question):\n",
        "    # Check if vector files exist\n",
        "    if not os.path.exists(\"vectors.index\") or not os.path.exists(\"chunks.pkl\"):\n",
        "        print(\"Error: Vector database not found!\")\n",
        "        print(\"run 'pdf_to_vectors.py' first to create the database.\")\n",
        "        return None\n",
        "\n",
        "    try:\n",
        "        # Load saved data\n",
        "        index = faiss.read_index(\"vectors.index\")\n",
        "        with open(\"chunks.pkl\", \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        chunks = data['chunks']\n",
        "        metadata = data['metadata']\n",
        "        total_pages = data['total_pages']\n",
        "\n",
        "        # Get question embedding from Gemini\n",
        "        response = genai.embed_content(\n",
        "            model=\"models/embedding-001\",\n",
        "            content=question,\n",
        "            output_dimensionality=1536\n",
        "        )\n",
        "        query_vector = np.array(response['embedding']).reshape(1, -1)\n",
        "\n",
        "        # Search similar chunks\n",
        "        scores, indices = index.search(query_vector.astype('float32'), 3)\n",
        "\n",
        "        print(f\"Found {len(indices[0])} relevant chunks:\")\n",
        "        for i, (score, idx) in enumerate(zip(scores[0], indices[0])):\n",
        "            page_num = metadata[idx]['estimated_page']\n",
        "            print(f\"   Chunk {i + 1}: Score {score:.3f} (â‰ˆPage {page_num})\")\n",
        "\n",
        "        # Build context\n",
        "        context_parts = []\n",
        "        for idx in indices[0]:\n",
        "            chunk_text = chunks[idx]\n",
        "            page_num = metadata[idx]['estimated_page']\n",
        "            context_parts.append(f\"[Page {page_num}]: {chunk_text}\")\n",
        "\n",
        "        context = '\\n\\n'.join(context_parts)\n",
        "\n",
        "        # Ask Gemini to answer with context\n",
        "        model = genai.GenerativeModel(\"gemini-1.5-flash\")\n",
        "        prompt = f\"\"\"\n",
        "You are answering questions about a {total_pages}-page document.\n",
        "When possible, mention the page numbers where the answer is found.\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer based on the context:\n",
        "\"\"\"\n",
        "        response = model.generate_content(prompt)\n",
        "        return response.text\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing question: {str(e)}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Check if database exists\n",
        "    if not os.path.exists(\"vectors.index\") or not os.path.exists(\"chunks.pkl\"):\n",
        "        print(\"Vector database not found!\")\n",
        "        print(\"Please run 'pdf_to_vectors.py' first to create the database.\")\n",
        "        print(\"Steps:\")\n",
        "        print(\"   1. Run: python pdf_to_vectors.py\")\n",
        "        print(\"   2. Then run: python ask_questions.py\")\n",
        "        return\n",
        "\n",
        "    # Load database info\n",
        "    try:\n",
        "        index = faiss.read_index(\"vectors.index\")\n",
        "        with open(\"chunks.pkl\", \"rb\") as f:\n",
        "            data = pickle.load(f)\n",
        "\n",
        "        chunks = data['chunks']\n",
        "        total_pages = data['total_pages']\n",
        "\n",
        "        print(f\"âœ… Database loaded: {len(chunks)} chunks from {total_pages} pages\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading database: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Interactive Q&A loop\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"ðŸ¤– RAG System Ready! Ask me questions about your PDF\")\n",
        "    print(\"ðŸ’¡ Type 'bye', 'quit', 'exit', or 'q' to exit\")\n",
        "    print(\"ðŸ”¢ Type 'info' to see database statistics\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    while True:\n",
        "        question = input(\"\\nYour question: \").strip()\n",
        "\n",
        "        if question.lower() in ['bye', 'quit', 'exit', 'q']:\n",
        "            print(\"Goodbye! Thanks for using the RAG system!\")\n",
        "            break\n",
        "\n",
        "        if question.lower() == 'info':\n",
        "            print(f\"ðŸ“Š Database Info:\")\n",
        "            print(f\"   â€¢ Total pages: {total_pages}\")\n",
        "            print(f\"   â€¢ Total chunks: {len(chunks)}\")\n",
        "            print(f\"   â€¢ Vector dimensions: 1536\")\n",
        "            print(f\"   â€¢ Average chunks per page: {len(chunks) / total_pages:.1f}\")\n",
        "            print(f\"   â€¢ Sample chunk: {chunks[0][:100]}...\")\n",
        "            continue\n",
        "\n",
        "        if not question:\n",
        "            print(\"Please enter a question!\")\n",
        "            continue\n",
        "\n",
        "        print(\"Searching and generating answer...\")\n",
        "        answer = ask_question(question)\n",
        "\n",
        "        if answer:\n",
        "            print(f\"Answer: {answer}\")\n",
        "        else:\n",
        "            print(\"Sorry, I couldn't generate an answer. Please try again.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rTb73x4vzQ2h",
        "outputId": "6ef80545-24d6-42e2-83b1-54223be5a01e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Database loaded: 28 chunks from 15 pages\n",
            "\n",
            "============================================================\n",
            "ðŸ¤– Gemini RAG System Ready! Ask me questions about your PDF\n",
            "ðŸ’¡ Type 'bye', 'quit', 'exit', or 'q' to exit\n",
            "ðŸ”¢ Type 'info' to see database statistics\n",
            "============================================================\n",
            "\n",
            "Your question: whats the document about\n",
            "Searching and generating answer...\n",
            "Found 3 relevant chunks:\n",
            "   Chunk 1: Score 0.578 (â‰ˆPage 10)\n",
            "   Chunk 2: Score 0.576 (â‰ˆPage 11)\n",
            "   Chunk 3: Score 0.563 (â‰ˆPage 10)\n",
            "Answer: Based on pages 10 and 11, the document appears to describe a dataset and a proposed technology for classifying cardiovascular disease (CVD).  The dataset includes patient information such as weight, gender, blood pressure, cholesterol levels, glucose levels, smoking status, alcohol consumption, and physical activity (page 10).  The proposed technology is a fuzzy-hybrid classification algorithm that uses this data to predict the presence or absence of CVD (pages 10-11).\n",
            "\n",
            "\n",
            "Your question: who is the author\n",
            "Searching and generating answer...\n",
            "Found 3 relevant chunks:\n",
            "   Chunk 1: Score 0.527 (â‰ˆPage 15)\n",
            "   Chunk 2: Score 0.527 (â‰ˆPage 2)\n",
            "   Chunk 3: Score 0.526 (â‰ˆPage 9)\n",
            "Answer: The provided text does not name the author(s) of the 15-page document.\n",
            "\n",
            "\n",
            "Your question: summarize tye content in the doc\n",
            "Searching and generating answer...\n",
            "Found 3 relevant chunks:\n",
            "   Chunk 1: Score 0.596 (â‰ˆPage 15)\n",
            "   Chunk 2: Score 0.572 (â‰ˆPage 11)\n",
            "   Chunk 3: Score 0.572 (â‰ˆPage 13)\n",
            "Answer: This 15-page document discusses cancer prediction models.  Page 11 details a feature selection method using fuzzy mutual information:  calculating membership degrees for data points and features (step 2), determining joint membership functions (step 3), computing fuzzy mutual information (step 4), ranking features based on these scores (step 5), and selecting the top k features (step 6). Page 13 presents performance metrics (accuracy, precision, recall, F1-score, MSE, RMSE, MAE, and binary loss) for several baseline algorithms: Decision Tree, NaÃ¯ve Bayes, SVM, XGBoost, Catboost, and Light GBM.  The document concludes (Page 15) by suggesting that ensemble approaches and improved interpretability of hybrid models could further enhance cancer prediction model performance.\n",
            "\n",
            "\n",
            "Your question: how may models used ?\n",
            "Searching and generating answer...\n",
            "Found 3 relevant chunks:\n",
            "   Chunk 1: Score 0.622 (â‰ˆPage 13)\n",
            "   Chunk 2: Score 0.620 (â‰ˆPage 13)\n",
            "   Chunk 3: Score 0.613 (â‰ˆPage 12)\n",
            "Answer: Based on the provided text (pages 12 and 13), six machine learning models were used: Decision Tree, NaÃ¯ve Bayes, SVM, XGBoost, Catboost, and Light GBM.\n",
            "\n",
            "\n",
            "Your question: traditional or they used any new methodolgies\n",
            "Searching and generating answer...\n",
            "Found 3 relevant chunks:\n",
            "   Chunk 1: Score 0.635 (â‰ˆPage 14)\n",
            "   Chunk 2: Score 0.628 (â‰ˆPage 13)\n",
            "   Chunk 3: Score 0.613 (â‰ˆPage 13)\n",
            "Answer: The provided text focuses on the performance of several algorithms (Decision Tree, NaÃ¯ve Bayes, SVM, XGBoost, Catboost, Light GBM)  and compares their performance metrics (MSE, RMSE, MAE, Binary Loss, Accuracy, Precision, Recall, F1-Score).  There is no mention of any new methodologies being used.  The document appears to be an analysis of existing algorithms. (Pages 13 and 14).\n",
            "\n",
            "\n",
            "Your question: are they ussing fuzzy mechanism\n",
            "Searching and generating answer...\n",
            "Found 3 relevant chunks:\n",
            "   Chunk 1: Score 0.689 (â‰ˆPage 11)\n",
            "   Chunk 2: Score 0.662 (â‰ˆPage 3)\n",
            "   Chunk 3: Score 0.659 (â‰ˆPage 11)\n",
            "Answer: Yes, the document explicitly states that the research introduces a \"Fuzzy Mutual Information -Based Hybrid Classification Algorithm\" (Page 3) and details the algorithm's use of fuzzy membership functions (Page 11).  The algorithm's procedure (Page 11) includes steps for computing fuzzy membership functions for features and the target variable, calculating the degree of membership, and computing fuzzy mutual information.\n",
            "\n",
            "\n",
            "Your question: top accuracy of the model\n",
            "Searching and generating answer...\n",
            "Found 3 relevant chunks:\n",
            "   Chunk 1: Score 0.640 (â‰ˆPage 13)\n",
            "   Chunk 2: Score 0.626 (â‰ˆPage 15)\n",
            "   Chunk 3: Score 0.620 (â‰ˆPage 15)\n",
            "Answer: Based on the provided text (page 13), the XGBoost model achieved the top accuracy of 82.1%.\n",
            "\n",
            "\n",
            "Your question: will they work ?\n",
            "Searching and generating answer...\n",
            "Found 3 relevant chunks:\n",
            "   Chunk 1: Score 0.547 (â‰ˆPage 13)\n",
            "   Chunk 2: Score 0.544 (â‰ˆPage 11)\n",
            "   Chunk 3: Score 0.544 (â‰ˆPage 13)\n",
            "Answer: The provided text focuses on the performance of various machine learning algorithms (Decision Tree, NaÃ¯ve Bayes, SVM, XGBoost, Catboost, Light GBM)  with and without a Fuzzy-Hybrid Classification method (FMI).  The algorithms' performance is measured using metrics such as Accuracy, Precision, Recall, F1-Score, MSE, RMSE, MAE, and Binary Loss (pages 13).  The document shows that these algorithms achieve varying degrees of success, with XGBoost and SVM consistently performing well, indicated by higher accuracy and F1-scores.  Whether they \"work\" depends on the specific application and the acceptable performance thresholds.  The document provides quantitative results (pages 13) to help assess their efficacy.\n",
            "\n",
            "\n",
            "Your question: bye\n",
            "Goodbye! Thanks for using the Gemini RAG system!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fYTt2P-C0wzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}